{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_xls = pd.read_excel('Training data.xlsx', 'Info', index_col=None)\n",
    "data_xls.to_csv('Info.csv', encoding='utf-8',index = False)\n",
    "data_xls = pd.read_excel('Training data.xlsx', 'TPR', index_col=None)\n",
    "data_xls.to_csv('TPR.csv', encoding='utf-8', index = False)\n",
    "\n",
    "infoSet = pd.read_csv(\"Info.csv\", header = None, skiprows=[0])\n",
    "TPRSet = pd.read_csv(\"TPR.csv\", header = None, skiprows=[0])\n",
    "\n",
    "# TPRSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge info set and TPR set\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "aggregation_functions = {2: 'mean', 3: 'mean', 4: 'mean', 5: 'mean', 6: 'mean'}\n",
    "\n",
    "TPRaggreg = TPRSet.groupby(TPRSet[0]).aggregate(aggregation_functions)\n",
    "mergeSet = pd.merge(infoSet.iloc[:, [True, True, True, False, False, False, False]], TPRaggreg.iloc[:, [True, True, True, True, True]], on=0)\n",
    "# mergeSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature list and clean strings\n",
    "import numpy as np\n",
    "\n",
    "def str_trim(series):\n",
    "    series = series.replace(' ', np.nan)\n",
    "    series = series.replace('', np.nan, regex=True)\n",
    "    series = series.replace(\"0\", np.nan, regex=True)\n",
    "    \n",
    "    feature = pd.DataFrame(series.str.split(',').explode())\n",
    "    feature = feature.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    feature = feature.replace('', np.nan)\n",
    "    feature = feature.dropna()\n",
    "\n",
    "    return series, feature\n",
    "\n",
    "bacteria = infoSet.iloc[:, 5]\n",
    "bacteria = bacteria.rename(\"bacteria\")\n",
    "bacteriaSet, bacteria_feature= str_trim(bacteria)\n",
    "bacteria_feature = bacteria_feature.to_numpy().flatten()\n",
    "\n",
    "common = infoSet.iloc[:, 3]\n",
    "common = common.rename(\"common\")\n",
    "commonSet, commonFeature = str_trim(common)\n",
    "commonFeature = commonFeature.to_numpy().flatten()\n",
    "# mergeSet[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one hot encoding\n",
    "def extract_df(ser, features):\n",
    "    df= pd.DataFrame()\n",
    "    for feature in features:\n",
    "        id = 0\n",
    "        new_list = []\n",
    "        for element in ser:\n",
    "            if feature == element:\n",
    "                new_list.append(1)\n",
    "            else:\n",
    "                new_list.append(0)\n",
    "    \n",
    "        df[feature] = new_list\n",
    "    return df\n",
    "            \n",
    "#origin_ant\n",
    "bacteria_df = extract_df(bacteriaSet, bacteria_feature)\n",
    "common_df = extract_df(commonSet, commonFeature)\n",
    "# common_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create cleaned data set\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# 創造 dummy variables\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "encoded_no = label_encoder.fit_transform(mergeSet[0])\n",
    "\n",
    "# 建立訓練與測試資料\n",
    "patient_x = pd.concat([mergeSet, bacteria_df, common_df], axis=1)\n",
    "patient_x = patient_x.iloc[:, 1:]\n",
    "patient_y = infoSet.iloc[:, 6]\n",
    "patient_y = np.array(patient_y)\n",
    "patient_y\n",
    "# type(patient_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min max scaler seems to be unhelpful\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(patient_x)\n",
    "# print(scaler.data_max_)\n",
    "# patient_x = scaler.transform(patient_x)\n",
    "# print(patient_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 44.        ,  87.76470588,  21.29411765, 113.05555556,\n",
       "         67.82352941],\n",
       "       [ 75.        ,  84.68421053,  19.75      , 131.78947368,\n",
       "         70.68421053],\n",
       "       [ 92.        ,  66.77777778,  18.3125    , 141.44444444,\n",
       "         65.16666667],\n",
       "       ...,\n",
       "       [ 64.        ,  91.41176471,  19.26666667, 138.6875    ,\n",
       "         74.5       ],\n",
       "       [ 57.        , 103.18518519,  18.        , 172.44444444,\n",
       "        107.85185185],\n",
       "       [ 82.        ,  75.7       ,  18.52941176, 125.52631579,\n",
       "         76.52631579]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce demensionality\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, SelectFpr, mutual_info_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(0.3))\n",
    "sel.fit_transform(patient_x)\n",
    "\n",
    "# Min max scaler seems to be unhelpful\n",
    "# kb = SelectKBest(mutual_info_classif, k=10).fit(patient_x, patient_y)\n",
    "# patient_x = kb.transform(patient_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate training set and testing set\n",
    "\n",
    "from sklearn import model_selection\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(patient_x, patient_y, test_size = 0.2)\n",
    "# Transform it to np array, for convenience\n",
    "# train_y = np.array(train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Logistic regression\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z, dtype=np.float128))\n",
    "\n",
    "def average_gradient(model, train_x, train_y):\n",
    "    gradient = np.zeros(len(model))\n",
    "    for index in range(len(train_x)):\n",
    "        gradient += np.multiply((sigmoid(model.T.dot(train_x[index])) - train_y[index]), train_x[index])\n",
    "    return gradient / len(gradient)\n",
    "\n",
    "def logistic_train(train_x, train_y):\n",
    "    model = np.zeros(len(train_x[0]))\n",
    "    \n",
    "    learning_rate = 1\n",
    "    limit = 1000\n",
    "    for i in range(limit):\n",
    "        model = model - learning_rate * average_gradient(model, train_x, train_y)\n",
    "        learning_rate *= 0.97 \n",
    "    return model\n",
    "\n",
    "def logistic_predict(model, data):\n",
    "    result = []\n",
    "    for x in data:\n",
    "        if sigmoid(model.dot(x)) > 0.7:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result\n",
    "\n",
    "def logistic_predict_prob(model, data):\n",
    "    result = []\n",
    "    for x in data:\n",
    "        result.append(sigmoid(model.dot(x)))\n",
    "    return result\n",
    "\n",
    "def visulization(model):\n",
    "    pass\n",
    "\n",
    "# model = logistic_train(train_x, train_y)\n",
    "# print(model)\n",
    "# test_y_predicted = logistic_predict(model, test_x)\n",
    "\n",
    "\n",
    "# thresholds = np.linspace(0,1,11)\n",
    "# k_scores = []\n",
    "# for threshold in thresholds:\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k_number)\n",
    "#     scores = cross_val_score(knn,X,y,cv=10,scoring='accuracy')\n",
    "#     k_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-eaa9753efbce>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z, dtype=np.float128))\n",
      "<ipython-input-9-eaa9753efbce>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z, dtype=np.float128))\n",
      "<ipython-input-9-eaa9753efbce>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z, dtype=np.float128))\n",
      "<ipython-input-9-eaa9753efbce>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z, dtype=np.float128))\n",
      "<ipython-input-9-eaa9753efbce>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z, dtype=np.float128))\n",
      "<ipython-input-9-eaa9753efbce>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z, dtype=np.float128))\n",
      "<ipython-input-9-eaa9753efbce>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z, dtype=np.float128))\n",
      "<ipython-input-9-eaa9753efbce>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z, dtype=np.float128))\n",
      "<ipython-input-9-eaa9753efbce>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z, dtype=np.float128))\n",
      "<ipython-input-9-eaa9753efbce>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z, dtype=np.float128))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import metrics\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(patient_x, patient_y)\n",
    "\n",
    "# print(skf.split(patient_x, patient_y))\n",
    "\n",
    "thresholds = np.linspace(0.1,1,10)\n",
    "# k_scores = []\n",
    "# for threshold in thresholds:\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k_number)\n",
    "#     scores = cross_val_score(knn,X,y,cv=10,scoring='accuracy')\n",
    "#     k_scores.append(scores.mean())\n",
    "\n",
    "k_scores = [0]\n",
    "best_threshold = 0\n",
    "for threshold in thresholds:\n",
    "    selector = VarianceThreshold(threshold=(threshold))\n",
    "    selected_patient_x = selector.fit_transform(patient_x)\n",
    "    f1s = []\n",
    "    for train_index, test_index in skf.split(selected_patient_x, patient_y):\n",
    "    #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        model = logistic_train(selected_patient_x[train_index], patient_y[train_index])\n",
    "        test_y_predicted = logistic_predict(model, selected_patient_x[test_index])\n",
    "        f1s.append(metrics.f1_score(patient_y[test_index], test_y_predicted))\n",
    "    if np.average(f1s) > np.max(k_scores):\n",
    "        best_threshold = threshold\n",
    "    k_scores.append(np.average(f1s))\n",
    "\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "print(best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0.4468159718159718,\n",
       " 0.4468159718159718,\n",
       " 0.43536250153897216,\n",
       " 0.43536250153897216,\n",
       " 0.43536250153897216,\n",
       " 0.43536250153897216,\n",
       " 0.43536250153897216,\n",
       " 0.43536250153897216,\n",
       " 0.43536250153897216,\n",
       " 0.43536250153897216]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.640625\n",
      "0.2580645161290323\n"
     ]
    }
   ],
   "source": [
    "# Validate accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "# Score\n",
    "accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "print(accuracy)\n",
    "# f1 score\n",
    "f1 = metrics.f1_score(test_y, test_y_predicted)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78125\n",
      "0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniellin/Desktop/homework/dm_pj1/ENV/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing, linear_model\n",
    "logistic_regr = linear_model.LogisticRegression()\n",
    "logistic_regr.fit(train_x, train_y)\n",
    "\n",
    "# 印出係數\n",
    "# print(logistic_regr.coef_)\n",
    "\n",
    "# 印出截距\n",
    "# print(logistic_regr.intercept_ )\n",
    "test_y_predicted = logistic_regr.predict(test_x)\n",
    "\n",
    "accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "print(accuracy)\n",
    "# f1 score\n",
    "f1 = metrics.f1_score(test_y, test_y_predicted)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-434d0f9cfc23>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-434d0f9cfc23>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    =(0))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()    #  回覆預設的RC引數\n",
    "fig,ax =plt.subplots()    #   ax為子圖\n",
    "\n",
    "features_label = list(range(1, len(model) + 1))\n",
    "y_pos = np.arange(len(model))\n",
    "=(0))\n",
    "# patient_x = sel.fit_transform(pat\n",
    "# exp_model = np.log(model)\n",
    "ax.barh(y_pos, model, align='center',color='green',ecolor='black')\n",
    "\n",
    "ax.set_yticks(y_pos)   #   設定縱座標的刻度\n",
    "ax.set_yticklabels(features_label) #   設定縱座標的標籤(人名)\n",
    "ax.invert_yaxis()  #   把Y反轉,取消這一行執行一下就明白了\n",
    "ax.set_xlabel('Feature Weight')   #   顯示X軸標籤\n",
    "\n",
    "plt.show()  #  顯示圖片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import metrics, svm\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # Create models\n",
    "# forest = RandomForestClassifier(n_estimators = 100)\n",
    "# extraTrees = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "# svc = svm.SVC(probability=True)\n",
    "# voting = VotingClassifier(estimators=[('rf', forest), ('et', extraTrees), ('svm', svc)], voting='soft')\n",
    "\n",
    "# for clf, label in zip([forest, extraTrees, svc, voting], ['Random Forest', 'Extra Trees', 'SVM', 'Voting Classifier']):\n",
    "#     scores = cross_val_score(clf, patient_x, patient_y, cv=5, scoring='accuracy')\n",
    "#     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    \n",
    "# voting_fit = voting.fit(train_x, train_y)\n",
    "\n",
    "# # Predict\n",
    "# test_y_predicted = voting.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cal AUC\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(test_y, test_y_predicted)\n",
    "# auc = metrics.auc(fpr, tpr)\n",
    "# print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # f1 score\n",
    "# f1 = metrics.f1_score(test_y, test_y_predicted)\n",
    "# f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# data_xls = pd.read_excel('Testing data.xlsx', 'Info', index_col=None)\n",
    "# data_xls.to_csv('TestingInfo.csv', encoding='utf-8',index = False)\n",
    "# data_xls = pd.read_excel('Testing data.xlsx', 'TPR', index_col=None)\n",
    "# data_xls.to_csv('TestingTPR.csv', encoding='utf-8', index = False)\n",
    "\n",
    "# infoSet = pd.read_csv(\"TestingInfo.csv\", header = None, skiprows=[0])\n",
    "# TPRSet = pd.read_csv(\"TestingTPR.csv\", header = None, skiprows=[0])\n",
    "\n",
    "# aggregation_functions = {2: 'mean', 3: 'mean', 4: 'mean', 5: 'mean', 6: 'mean'}\n",
    "\n",
    "# TPRaggreg = TPRSet.groupby(TPRSet[0]).aggregate(aggregation_functions)\n",
    "# mergeSet = pd.merge(infoSet.iloc[:, [True, True, True, False, False, False]], TPRaggreg.iloc[:, [True, True, True, True, True]], on=0)\n",
    "\n",
    "# # Extract feature\n",
    "# bacteria = infoSet.iloc[:, 5]\n",
    "# bacteria = bacteria.rename(\"bacteria\")\n",
    "# bacteriaSet, bacteria_feature_not_used = str_trim(bacteria)\n",
    "# # bacteria_feature_not_used = bacteria_feature_not_used.to_numpy().flatten()\n",
    "\n",
    "# common = infoSet.iloc[:, 3]\n",
    "# common = common.rename(\"common\")\n",
    "# commonSet, commonFeature = str_trim(common)\n",
    "# commonFeature = commonFeature.to_numpy().flatten()\n",
    "\n",
    "# # One Hot encoding\n",
    "# bacteria_df = extract_df(bacteriaSet, bacteria_feature)\n",
    "# common_df = extract_df(commonSet, commonFeature)\n",
    "\n",
    "# # Concate\n",
    "# test_x = pd.concat([mergeSet, bacteria_df, common_df], axis=1)\n",
    "# test_x = test_x.iloc[:, 1:]\n",
    "\n",
    "# # Cover select k best mask\n",
    "# mask = kb.get_support()\n",
    "\n",
    "# test_x = test_x.iloc[:,mask]\n",
    "\n",
    "# test_y_predicted = voting.predict(test_x)\n",
    "\n",
    "# test_y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = np.array([[15, 2], [3, 4], [15, 2], [3, 4]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=2)\n",
    "# skf.get_n_splits(X, y)\n",
    "\n",
    "# print(skf.split(X, y))\n",
    "\n",
    "# for train_index, test_index in skf.split(X, y):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
